{"cells":[{"cell_type":"code","metadata":{"source_hash":"9cfa2309","execution_start":1687709609765,"execution_millis":3610,"deepnote_to_be_reexecuted":false,"cell_id":"a0d693533b5c4b17a64a2684c016af77","deepnote_cell_type":"code"},"source":"!pip install openai","execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai in /root/venv/lib/python3.9/site-packages (0.27.8)\nRequirement already satisfied: aiohttp in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from openai) (3.8.3)\nRequirement already satisfied: requests>=2.20 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from openai) (2.28.1)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from openai) (4.64.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.20->openai) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.20->openai) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (6.0.2)\nRequirement already satisfied: attrs>=17.3.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (22.1.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"190021c","execution_start":1687709626529,"execution_millis":1,"deepnote_to_be_reexecuted":false,"cell_id":"aa5de77184d745148582c86b3486a7fe","deepnote_cell_type":"code"},"source":"# Import Chat completion template and set-up variables\n\nimport openai\nimport urllib.parse\n\nopenai.api_key = \"EMPTY\" # Key is ignored and does not matter\nopenai.api_base = \"http://34.132.127.197:8000/v1\"\n\n# Report issues\ndef raise_issue(e, model, prompt):\n    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n\n# Query Gorilla server\ndef get_gorilla_response(prompt=\"I would like to translate from English to French.\", model=\"gorilla-7b-hf-v0\"):\n  try:\n    completion = openai.ChatCompletion.create(\n      model=model,\n      messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return completion.choices[0].message.content\n  except Exception as e:\n    raise_issue(e, model, prompt)","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"ec321ad0","execution_start":1687710023964,"execution_millis":23826,"deepnote_to_be_reexecuted":false,"cell_id":"0c5f334c41a2467ca68bfcccd1a58764","deepnote_cell_type":"code"},"source":"# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n# Translation\nprompt = \"I would like to translate 'I feel elated today' from English to Hindi.\"\nprint(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))","execution_count":9,"outputs":[{"name":"stdout","text":"<<<domain>>>: Natural Language Processing Text2Text Generation\n<<<api_call>>>: model = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>: 1. We import the necessary components from the Hugging Face Transformers library.\n2. We specify the source language (English) and target language (Hindi) for the translation.\n3. We initialize the tokenizer and model using the pretrained 'facebook/m2m100_418M' model.\n4. We tokenize the input text 'I feel elated today' and generate the translated text tokens using the model.\n5. We decode the generated tokens back to the text in the target language (Hindi) using the tokenizer.\n<<<code>>>:\nfrom transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\ndef load_model():\n    model = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_418M')\n    tokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_418M')\n    return model, tokenizer\n\ndef process_data(text, src_lang, tgt_lang, model, tokenizer):\n    tokenizer.src_lang = src_lang\n    encoded_input = tokenizer(text, return_tensors='pt')\n    generated_tokens = model.generate(**encoded_input, forced_bos_token_id=tokenizer.get_lang_id(tgt_lang))\n    response = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n    return response[0]\n\ntext = 'I feel elated today'\nsrc_lang = 'en_XX'\ntgt_lang = 'hi_IN'\n\n# Load the model and tokenizer\nmodel, tokenizer = load_model()\n\n# Process the data\nresponse = process_data(text, src_lang, t\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":"2fddb2b6","execution_start":1687710274200,"execution_millis":17109,"deepnote_to_be_reexecuted":false,"cell_id":"1b2c6e9440094d938c5db645265b0f84","deepnote_cell_type":"code"},"source":"# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n# Translation\nprompt = \"convert 'what is the meaning of semantic search' to vectors\"\nprint(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))","execution_count":12,"outputs":[{"name":"stdout","text":"<<<domain>>>: Natural Language Processing Conversational\n<<<api_call>>>: model = AutoModel.from_pretrained('satvikag/chatbot')\n<<<api_provider>>>: PyTorch Transformers\n<<<explanation>>>: 1. Import the required libraries and components from the PyTorch Transformers library.\n2. Load the pretrained GPT-2 model 'satvikag/chatbot' for conversation.\n3. Tokenize the input text for semantic search conversion.\n4. Generate embeddings for the tokenized input.\n5. Convert the embeddings back to a readable format.<<<code>>>:\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\ndef load_model():\n    tokenizer = AutoTokenizer.from_pretrained('satvikag/chatbot')\n    model = AutoModel.from_pretrained('satvikag/chatbot')\n    return tokenizer, model\n\ndef process_data(input_text, tokenizer, model):\n    input_tokens = tokenizer.encode(input_text, return_tensors='pt')\n    embeddings = model.generate(input_tokens, max_length=50, num_return_sequences=1)\n    response = embeddings[0].tolist()\n    return response\n\ninput_text = \"what is the meaning of semantic search\"\n\n# Load the model and tokenizer\ntokenizer, model = load_model()\n\n# Process the data\nresponse = process_data(input_text, tokenizer, model)\nprint(response)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e548e5e9-a544-46de-9965-90d8f8633719' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"17b1e386752e47368813f1c1a769df98","deepnote_execution_queue":[]}}